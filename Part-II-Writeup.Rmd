---
title: "Part1 Analysis"
author: "Team: SparkR"
date: "2017/12/7"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(tidyr))
suppressMessages(library(gridExtra))
suppressMessages(library(grid))
suppressMessages(library(knitr))
suppressMessages(library(xgboost))
```

1. Introduction: Summary of problem and objectives (5 points)

Late 18th century Paris has witnessed countless number of auctions on fine arts and paintings from countries across Europe. Our project utilizes this dataset from auction in 18th century Paris with details of paintings, authors and auctions. 
We hope to build a OLS model to predict the log of price fetched at auction. Since our dataset contains 59 variables, we would want to perform variable selection to build a parsimonious model with good performance in critiria such as RMSE, etc. 
In addition to that, some variables have a lot of missing values and typos. We would need to perform some sort of missing imputation, e.g. assume MCAR and use MICE package. For typos, we would recode them to appropriate values.
From the correlation martices, we find some highly-correlated variables, so we decide to not include some of the variables in our model to aviod multicollinearity.

2. Exploratory data analysis (10 points): must include three correctly labeled graphs and an explanation that highlight the most important features that went into your model building.


```{r echo=FALSE, warning=FALSE}
###Function to clean the dataset

#load the training data set
load("paintings_train.Rdata")
load("paintings_test.Rdata")

tt = paintings_train

clean = function(df, NA.omit=F){
   df = df %>% 
     select(-c(sale, price, count, subject, winningbidder, authorstandard, authorstyle, author, Height_in, Width_in, Surface_Rect, Diam_in, winningbiddertype, Surface_Rnd,material,materialCat, Interm)) %>%
     mutate(lot = as.numeric(lot)) %>%
     mutate(mat_recode = ifelse(mat %in% c("a", "bc", "c"), "metal",
                          ifelse(mat %in% c("al", "ar", "m"), "stone",
                                ifelse(mat %in% c("co", "bt", "t"), "canvas",
                                       ifelse(mat %in% c("p", "ca"), "paper",
                                              ifelse(mat %in% c("b"), "wood",
                                                     ifelse(mat %in% c("o", "e", "v"), "other",
                                                            ifelse(mat %in% c("n/a", ""), "na",
                                                                   "na")))))))) %>%
     mutate(school_pntg_recode = ifelse(school_pntg %in% c("A", "G", "S"), "other",school_pntg)) %>%
     mutate(SurfaceNA = ifelse(is.na(df$Surface), 1, 0))%>%
     mutate(nfigures=as.numeric(nfigures))%>%
     mutate(Surface=log(Surface+1))%>%
     mutate(endbuyer = ifelse(endbuyer == "", "NA", endbuyer)) %>% 
   mutate(shape_recode = ifelse(Shape == "", "Other",
                               ifelse(Shape == "ovale", "oval",
                                      ifelse(Shape == "ronde", "round",
                                             ifelse(Shape == "octogon", "Other", Shape)))))%>% 
     select(-c(mat,school_pntg, Shape))
   
   factornames=colnames(df)[!colnames(df) %in% c("logprice","lot","position","Surface","nfigures")]
   
   df[factornames] = lapply(df[factornames], factor)

   df$Surface[is.na(df$Surface)] = 0
  
  
   if(NA.omit==T){
      df = na.omit(df)
   }


   #df=df[vapply(df, function(x) length(unique(x)) > 1, logical(1L))]
   
  return(df) 
}


train = clean(paintings_train, NA.omit = T)
test = clean(paintings_test, NA.omit = F)





for(i in setdiff(1:ncol(train), c(2,14,15))) {
  levels(train[,i]) = union(levels(train[,i]), levels(test[,i]))
}

#levels(train$endbuyer) = levels(train$endbuyer)[-1]
#levels(test$endbuyer) = levels(test$endbuyer)[-1]

#train$winningbiddertype = paintings_train$winningbiddertype

#winningbiddertype
```


```{r}
library(tree)
library(rpart)
tree.nes=tree(logprice ~., data=train)
cv.vote = cv.tree(tree.nes, FUN=prune.tree)
prune.vote = prune.rpart(cv.vote,cp = 0.1)

fit <- rpart(logprice ~ ., data = train)
test =clean(paintings_test, NA.omit = F)
fit$xlevels[["winningbiddertype"]] <- union(fit$xlevels[["winningbiddertype"]], levels(test$winningbiddertype))
pred1=predict(fit,newdata = test, type="vector")
```


```{r}
library(BAS)
df.bas = bas.lm(logprice~Surface + dealer + endbuyer + diff_origin + type_intermed + engraved + mat_recode+school_pntg_recode+paired + lrgfont + lands_sc + othgenre + discauth + othartist+still_life, data=train,
prior="g-prior", a=nrow(train), modelprior=uniform(),
method="MCMC", MCMC.iterations = 2000000, thin = 20)

y.pred.bma = predict(df.bas, newdata = test, estimator = "BMA", se.fit = TRUE, prediction = FALSE)

predictions = confint(y.pred.bma, parm = "pred")
predictions = as.data.frame(matrix(predictions, nrow = nrow(test)))

colnames(predictions) = c("lwr", "upr", "fit")
predictions = predictions %>% 
  select(fit, lwr, upr) %>% 
  mutate(fit = exp(fit),
         lwr = exp(lwr),
         upr = exp(upr))

save(predictions, file = "predict-test.Rdata")
```

```{r}
library(gbm)

boost.logprice =gbm(logprice~Surface + dealer + endbuyer + diff_origin + type_intermed + engraved + mat_recode+school_pntg_recode+paired + lrgfont + lands_sc + othgenre + discauth + othartist+still_life, data=train, 
                distribution="gaussian",n.trees =10000,
                interaction.depth = 4)

pred = predict(boost.logprice,newdata=test,n.trees=10000,
                       type="response")
pred = exp(pred)
predictions = as.data.frame(pred)
colnames(predictions) = c("fit")
save(predictions, file = "predict-test.Rdata")
```

```{r}
library(caret)
#https://stackoverflow.com/questions/15613332/using-caret-package-to-find-optimal-parameters-of-gbm
getModelInfo()$gbm$parameters
library(parallel)
library(doMC)
registerDoMC(cores = 16)
# Max shrinkage for gbm
nl = nrow(train)
max(0.01, 0.1*min(1, nl/10000))
# Max Value for interaction.depth
floor(sqrt(NCOL(train)))
gbmGrid <-  expand.grid(interaction.depth = c(1,2,4,6),
                    n.trees = (0:100)*100, 
                    shrinkage = seq(.0005, .01,.0005),
                    n.minobsinnode = 10) # you can also put something        like c(5, 10, 15, 20)

fitControl <- trainControl(method = "repeatedcv",
                       repeats = 5,
                       preProcOptions = list(thresh = 0.95),
                       ## Estimate class probabilities
                       classProbs = TRUE,
                       ## Evaluate performance using
                       ## the following function
                       summaryFunction = twoClassSummary)

# Method + Date + distribution
set.seed(1)
system.time(GBM <- train(logprice ~ ., data = train,
            distribution = "gaussian",
            method = "gbm", bag.fraction = 0.5,
            nTrain = round(nrow(train) *.75),
            trControl = fitControl,
            verbose = TRUE,
            tuneGrid = gbmGrid,
            ## Specify which metric to optimize
            metric = "ROC"))
```


```{r}
#XGBoost
#using one hot encoding 
labels = train$logprice
ts_label = test$label
new_tr = model.matrix(~.+0,data = train[,-c("target"),with=F]) 
new_ts <- model.matrix(~.+0,data = test[,-c("target"),with=F])

#unfinished
#convert factor to numeric 
labels <- as.numeric(labels)-1
ts_label <- as.numeric(ts_label)-1
#add

```

