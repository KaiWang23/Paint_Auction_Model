---
title: "Part1 Analysis"
author: "Team: SparkR"
date: "2017/12/7"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(tidyr))
suppressMessages(library(gridExtra))
suppressMessages(library(grid))
```

1. Introduction: Summary of problem and objectives (5 points)

Late 18th century Paris has witnessed countless number of auctions on fine arts and paintings from countries across Europe. Our project utilizes this dataset from auction in 18th century Paris with details of paintings, authors and auctions. 
We hope to build a OLS model to predict the log of price fetched at auction. Since our dataset contains 59 variables, we would want to perform variable selection to build a parsimonious model with good performance in critiria such as RMSE, etc. 
In addition to that, some variables have a lot of missing values and typos. We would need to perform some sort of missing imputation, e.g. assume MCAR and use MICE package. For typos, we would recode them to appropriate values.
From the correlation martices, we find some highly-correlated variables, so we dicide to not include some of the variables in our model to aviod multicollinearity.


2. Exploratory data analysis (10 points): must include three correctly labeled graphs and an explanation that highlight the most important features that went into your model building.

```{r echo=FALSE}
###Function to clean the dataset

#load the training data set
load("paintings_train.Rdata")


clean = function(df, NA.omit=F){
   df = df %>% 
     select(-c(sale, price, count, subject, winningbidder, winningbiddertype, authorstandard, authorstyle, author, Height_in, Width_in, Surface_Rect, Diam_in, Surface_Rnd,material,materialCat)) %>%
     mutate(lot = as.numeric(lot)) %>%
     mutate(mat_recode = ifelse(mat %in% c("a", "bc", "c"), "metal",
                          ifelse(mat %in% c("al", "ar", "m"), "stone",
                                ifelse(mat %in% c("co", "bt", "t"), "canvas",
                                       ifelse(mat %in% c("p", "ca"), "paper",
                                              ifelse(mat %in% c("b"), "wood",
                                                     ifelse(mat %in% c("o", "e", "v"), "other",
                                                            ifelse(mat %in% c("n/a", ""), "na",
                                                                   "na")))))))) %>%
     mutate(school_pntg_recode = ifelse(school_pntg %in% c("A", "G", "S"), "other",school_pntg)) %>%
     mutate(SurfaceNA = ifelse(is.na(df$Surface), 1, 0))%>%
     mutate(IntermNA = ifelse(is.na(df$Interm), 1, 0)) %>%
     mutate(nfigures=as.numeric(nfigures))%>%
     mutate(Surface=log(Surface+1))%>%
     select(-c(mat,school_pntg))
   
   factornames=colnames(df)[!colnames(df) %in% c("logprice","lot","position","Surface","nfigures")]
   
   df[factornames] = lapply(df[factornames], factor)

   df$Surface[is.na(df$Surface)] = 0
  
  
   if(NA.omit==T){
      df = na.omit(df)
   }


   df=df[vapply(df, function(x) length(unique(x)) > 1, logical(1L))]
   
  return(df) 
}


train = clean(paintings_train, NA.omit = T)
```



For this part, we intially have found some interaction between predictors from graph perspective; however, in our further model selection process and trials and error.

First, we choose to plot boxplots for the variables that are highly correlated with response `logprice`

```{r echo=FALSE}
train_sub1 = train %>% select(logprice, dealer, endbuyer, type_intermed, lrgfont)


df1 = train %>% 
  select(logprice, dealer) 

df2 = train %>% 
  select(logprice, endbuyer)

df3 = train %>% 
  select(logprice, type_intermed)

df4 = train %>% 
  select(logprice, lrgfont) 

# df1.final = rbind(df1,df2,df3, df4)
# 
# ggplot(train, aes(x=type, y= logprice)) +
#   facet_wrap(~type) +
#   geom_boxplot()
```

```{r echo=FALSE}
g1 = ggplot(df1, aes(x=dealer, y= logprice)) +
  geom_boxplot(color="#999999",  alpha=0.8)
g2 = ggplot(df2, aes(x=endbuyer, y= logprice)) +
  geom_boxplot(color="#E69F00",  alpha=0.8)
g3 = ggplot(df3, aes(x=type_intermed, y= logprice)) +
  geom_boxplot(color="#56B4E9",  alpha=0.8)
g4 = ggplot(df4, aes(x=lrgfont, y= logprice)) +
  geom_boxplot(color="purple",  alpha=0.8)

grid.arrange(g1, g2,g3,g4, ncol=2, top=textGrob("Boxplot for 4 most 'significant' variables", gp=gpar(fontsize=13,font=8)))
```

3. Development and assessment of an initial model (10 points)

* Initial model: must include a summary table and an explanation/discussion for variable selection and overall amount of variation explained. 

* Model selection: must include a discussion

* Residual: must include residual plot(s) and a discussion.  

* Variables: must include table of coefficients and CI
```{r model1, echo=FALSE}
model1 = lm(logprice ~ 1+., data = train)

model1 = step(model1, k=log(nrow(train)), trace = 0)
results.bic=summary(model1)
results.bic$coefficients

train=clean(paintings_train, NA.omit = F)
model1 = lm(logprice~1 + Surface + dealer + endbuyer + diff_origin + type_intermed + engraved + mat_recode+school_pntg_recode+paired + lrgfont + lands_sc + othgenre + discauth + othartist+still_life,
    data = train)
```

```{r echo=FALSE}
#residual plots
par(mfrow = c(2,2))
plot(model1, c(1:4))
```

The Residual vs Fitted plot shows consistent residuals across different fitted values, which means constant variance assumption is satisfied. In addition to that, the Normal Q-Q plot displays essentially normal distributed residuals. And from Cook's distance plot, we can see all observations have small Cook's distance and there is no sign of high leverage points nor influential points. Hence, we decide that all assumptions for this model have been met and we will use this model for prediction.

The coefficients and confidence interval of variables used are as follows:

```{r echo=FALSE}
#coefficients and CI
CI = cbind(model1$coefficients,confint(model1))
colnames(CI) = c("coefficient","2.5%","97.5%")
kable(CI)
```

```{r predict-model1, echo=FALSE}
load("paintings_test.Rdata")
test =clean(paintings_test, NA.omit = F)

predictions = as.data.frame(
  exp(predict(model1, newdata=test, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```

4. Summary and Conclusions (10 points)

What is the (median) price for the "baseline" category if there are categorical or dummy variables in the model (add CI's)?  (be sure to include units!) Highlight important findings and potential limitations of your model.  Does it appear that interactions are important?  What are the most important variables and/or interactions?  Provide interprations of how the most important variables influence the (median) price giving a range (CI).  Correct interpretation of coefficients for the log model desirable for full points.

Provide recommendations for the art historian about features or combination of features to look for to find the most valuable paintings.



```{r}

```

